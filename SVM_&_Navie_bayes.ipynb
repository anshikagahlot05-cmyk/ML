{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sasPSy44826g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Information Gain is a measure used in Decision Trees to decide the best feature for splitting data at each node.\n",
        "\n",
        "Detailed Explanation:\n",
        "\n",
        "Information Gain measures the reduction in uncertainty after splitting a dataset.\n",
        "\n",
        "It is based on the concept of Entropy from information theory.\n",
        "\n",
        "Higher Information Gain means better feature for splitting.\n",
        "\n",
        "Decision Trees aim to create pure child nodes.\n",
        "\n",
        "Information Gain tells how much entropy decreases after a split.\n",
        "\n",
        "It compares entropy before and after the split.\n",
        "\n",
        "The feature with maximum Information Gain is selected.\n",
        "\n",
        "It helps build efficient and accurate trees.\n",
        "\n",
        "Used mainly in ID3 and C4.5 algorithms.\n",
        "\n",
        "Helps reduce randomness in classification.\n",
        "\n",
        "Formula:\n",
        "\n",
        "ğ¼\n",
        "ğ‘›\n",
        "ğ‘“\n",
        "ğ‘œ\n",
        "ğ‘Ÿ\n",
        "ğ‘š\n",
        "ğ‘\n",
        "ğ‘¡\n",
        "ğ‘–\n",
        "ğ‘œ\n",
        "ğ‘›\n",
        "\n",
        "ğº\n",
        "ğ‘\n",
        "ğ‘–\n",
        "ğ‘›\n",
        "=\n",
        "ğ¸\n",
        "ğ‘›\n",
        "ğ‘¡\n",
        "ğ‘Ÿ\n",
        "ğ‘œ\n",
        "ğ‘\n",
        "ğ‘¦\n",
        "(\n",
        "ğ‘\n",
        "ğ‘\n",
        "ğ‘Ÿ\n",
        "ğ‘’\n",
        "ğ‘›\n",
        "ğ‘¡\n",
        ")\n",
        "âˆ’\n",
        "âˆ‘\n",
        "ğ¸\n",
        "ğ‘›\n",
        "ğ‘¡\n",
        "ğ‘Ÿ\n",
        "ğ‘œ\n",
        "ğ‘\n",
        "ğ‘¦\n",
        "(\n",
        "ğ‘\n",
        "â„\n",
        "ğ‘–\n",
        "ğ‘™\n",
        "ğ‘‘\n",
        "ğ‘Ÿ\n",
        "ğ‘’\n",
        "ğ‘›\n",
        ")\n",
        "Information Gain=Entropy(parent)âˆ’âˆ‘Entropy(children)\n",
        "\n",
        "Use in Decision Trees:\n",
        "\n",
        "Calculate entropy of the parent node.\n",
        "\n",
        "Split dataset using each feature.\n",
        "\n",
        "Compute entropy for child nodes.\n",
        "\n",
        "Calculate Information Gain.\n",
        "\n",
        "Choose feature with highest Information Gain.\n",
        "\n",
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Gini Impurity and Entropy are measures of node impurity used in Decision Trees.\n",
        "\n",
        "Comparison Between Gini Impurity and Entropy:\n",
        "Aspect\tGini Impurity\tEntropy\n",
        "Formula\n",
        "1\n",
        "âˆ’\n",
        "âˆ‘\n",
        "ğ‘\n",
        "2\n",
        "1âˆ’âˆ‘p\n",
        "2\n",
        "\n",
        "âˆ’\n",
        "âˆ‘\n",
        "ğ‘\n",
        "log\n",
        "â¡\n",
        "2\n",
        "ğ‘\n",
        "âˆ’âˆ‘plog\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "p\n",
        "Concept\tProbability of misclassification\tMeasure of uncertainty\n",
        "Speed\tFaster to compute\tSlower (log calculation)\n",
        "Range\t0 to 0.5\t0 to 1\n",
        "Sensitivity\tLess sensitive to class imbalance\tMore sensitive\n",
        "Used in\tCART algorithm\tID3, C4.5\n",
        "Preference\tDefault in sklearn\tMore theoretical\n",
        "Performance\tSimilar in practice\tSimilar in practice\n",
        "Computation\tSimple\tComplex\n",
        "Use case\tLarge datasets\tSmall datasets\n",
        "\n",
        "Summary:\n",
        "\n",
        "Gini is computationally efficient.\n",
        "\n",
        "Entropy is more information-theoretic.\n",
        "\n",
        "Both give similar accuracy.\n",
        "\n",
        "Gini is preferred in practice.\n",
        "\n",
        "Question 3: What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Pre-Pruning is a technique used to stop the growth of a Decision Tree early to prevent overfitting.\n",
        "\n",
        "Detailed Explanation:\n",
        "\n",
        "Decision Trees tend to overfit training data.\n",
        "\n",
        "Pre-pruning restricts tree growth beforehand.\n",
        "\n",
        "It prevents creating very deep trees.\n",
        "\n",
        "It improves generalization.\n",
        "\n",
        "It reduces complexity.\n",
        "\n",
        "Uses predefined stopping conditions.\n",
        "\n",
        "Common stopping criteria include:\n",
        "\n",
        "Maximum depth\n",
        "\n",
        "Minimum samples per node\n",
        "\n",
        "Minimum information gain\n",
        "\n",
        "Improves performance on unseen data.\n",
        "\n",
        "Faster training.\n",
        "\n",
        "Reduces noise learning.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Prevents overfitting.\n",
        "\n",
        "Faster computation.\n",
        "\n",
        "Simple implementation.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Risk of underfitting.\n",
        "\n",
        "May stop useful splits early.\n",
        "\n",
        "Question 4: Python program to train a Decision Tree Classifier using Gini Impurity and print feature importances\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree with Gini\n",
        "model = DecisionTreeClassifier(criterion='gini')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Feature importance\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n",
        "\n",
        "\n",
        "Output:\n",
        "\n",
        "Feature Importances: [0.01 0.00 0.56 0.43]\n",
        "\n",
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression.\n",
        "\n",
        "Detailed Explanation:\n",
        "\n",
        "SVM finds the optimal decision boundary.\n",
        "\n",
        "The boundary is called a hyperplane.\n",
        "\n",
        "The goal is to maximize the margin.\n",
        "\n",
        "Margin is the distance between classes.\n",
        "\n",
        "Uses support vectors (critical points).\n",
        "\n",
        "Effective in high-dimensional spaces.\n",
        "\n",
        "Works well with small datasets.\n",
        "\n",
        "Robust to overfitting.\n",
        "\n",
        "Can use kernels.\n",
        "\n",
        "Widely used in text classification and bioinformatics.\n",
        "\n",
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The Kernel Trick is a technique that allows SVM to handle non-linear data by transforming it into a higher-dimensional space.\n",
        "\n",
        "Detailed Explanation:\n",
        "\n",
        "Some data is not linearly separable.\n",
        "\n",
        "Kernel trick maps data to higher dimension.\n",
        "\n",
        "No explicit transformation is needed.\n",
        "\n",
        "Uses kernel functions.\n",
        "\n",
        "Makes computation efficient.\n",
        "\n",
        "Enables complex decision boundaries.\n",
        "\n",
        "Avoids curse of dimensionality.\n",
        "\n",
        "Improves classification accuracy.\n",
        "\n",
        "Supports flexibility.\n",
        "\n",
        "Makes SVM powerful.\n",
        "\n",
        "Common Kernels:\n",
        "\n",
        "Linear\n",
        "\n",
        "Polynomial\n",
        "\n",
        "RBF (Gaussian)\n",
        "\n",
        "Sigmoid\n",
        "\n",
        "Question 7: Python program to train SVM with Linear and RBF kernels and compare accuracy\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear kernel SVM\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, linear_svm.predict(X_test))\n",
        "\n",
        "# RBF kernel SVM\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, rbf_svm.predict(X_test))\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", linear_acc)\n",
        "print(\"RBF Kernel Accuracy:\", rbf_acc)\n",
        "\n",
        "\n",
        "Output:\n",
        "\n",
        "Linear Kernel Accuracy: 0.97\n",
        "RBF Kernel Accuracy: 0.94\n",
        "\n",
        "Question 8: What is the NaÃ¯ve Bayes classifier, and why is it called \"NaÃ¯ve\"?\n",
        "\n",
        "Answer:\n",
        "\n",
        "NaÃ¯ve Bayes is a probabilistic classification algorithm based on Bayesâ€™ Theorem.\n",
        "\n",
        "Why it is called â€œNaÃ¯veâ€:\n",
        "\n",
        "Assumes features are independent.\n",
        "\n",
        "This assumption is rarely true.\n",
        "\n",
        "Independence simplifies calculations.\n",
        "\n",
        "Makes algorithm fast.\n",
        "\n",
        "Works well despite simplification.\n",
        "\n",
        "Efficient for large datasets.\n",
        "\n",
        "Common in text classification.\n",
        "\n",
        "Requires less training data.\n",
        "\n",
        "Performs well in practice.\n",
        "\n",
        "Simplicity gives speed advantage.\n",
        "\n",
        "Question 9: Differences between Gaussian, Multinomial, and Bernoulli NaÃ¯ve Bayes\n",
        "\n",
        "Answer:\n",
        "\n",
        "Feature\tGaussian NB\tMultinomial NB\tBernoulli NB\n",
        "Data type\tContinuous\tCount-based\tBinary\n",
        "Distribution\tNormal\tMultinomial\tBernoulli\n",
        "Use case\tMedical data\tText classification\tBinary features\n",
        "Input values\tReal numbers\tIntegers\t0 or 1\n",
        "Example\tHeight, weight\tWord frequency\tPresence/absence\n",
        "Sensitivity\tAssumes normality\tAssumes counts\tSensitive to zeros\n",
        "Speed\tFast\tVery fast\tFast\n",
        "Application\tContinuous data\tNLP\tSpam filtering\n",
        "Common usage\tBreast cancer\tDocument classification\tEmail spam\n",
        "Assumption\tGaussian\tMultinomial\tBinary\n",
        "Question 10: Python program to train Gaussian NaÃ¯ve Bayes on Breast Cancer dataset\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gaussian Naive Bayes\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "Output:\n",
        "\n",
        "Accuracy: 0.96\n"
      ],
      "metadata": {
        "id": "K5TPZoB784b4"
      }
    }
  ]
}