{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9HBuEVe-1jF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:\n",
        "What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "\n",
        "Answer:\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a supervised, instance-based, and non-parametric machine learning algorithm used for both classification and regression tasks.\n",
        "\n",
        "How KNN Works (General Working):\n",
        "\n",
        "KNN does not build an explicit model during training.\n",
        "\n",
        "It stores the entire training dataset.\n",
        "\n",
        "When a new data point is given, KNN calculates the distance between this point and all training points.\n",
        "\n",
        "It selects the K nearest data points based on distance.\n",
        "\n",
        "The prediction is made based on these neighbors.\n",
        "\n",
        "Common distance measures include Euclidean and Manhattan distance.\n",
        "\n",
        "Value of K is chosen by the user.\n",
        "\n",
        "Smaller K → sensitive to noise.\n",
        "\n",
        "Larger K → smoother decision boundary.\n",
        "\n",
        "KNN is simple yet powerful.\n",
        "\n",
        "KNN for Classification:\n",
        "\n",
        "The class labels of the K nearest neighbors are considered.\n",
        "\n",
        "Majority voting is applied.\n",
        "\n",
        "The class with the highest frequency is assigned.\n",
        "\n",
        "Works well for multi-class problems.\n",
        "\n",
        "Sensitive to feature scale.\n",
        "\n",
        "Requires feature normalization.\n",
        "\n",
        "Common in image and text classification.\n",
        "\n",
        "Performs well with well-separated classes.\n",
        "\n",
        "Prediction time is high.\n",
        "\n",
        "Accuracy depends on K and distance metric.\n",
        "\n",
        "KNN for Regression:\n",
        "\n",
        "The output values of K nearest neighbors are considered.\n",
        "\n",
        "The average (mean) of these values is calculated.\n",
        "\n",
        "The average becomes the predicted value.\n",
        "\n",
        "No voting mechanism is used.\n",
        "\n",
        "Sensitive to outliers.\n",
        "\n",
        "Suitable for smooth numerical data.\n",
        "\n",
        "Performs poorly with noisy data.\n",
        "\n",
        "Requires proper scaling.\n",
        "\n",
        "Computationally expensive.\n",
        "\n",
        "Simple to understand and implement.\n",
        "\n",
        "Question 2:\n",
        "What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The Curse of Dimensionality refers to the problems that arise when the number of features (dimensions) increases.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "As dimensions increase, data becomes sparse.\n",
        "\n",
        "Distance between data points becomes less meaningful.\n",
        "\n",
        "All points appear almost equally distant.\n",
        "\n",
        "KNN relies heavily on distance calculations.\n",
        "\n",
        "Nearest neighbors may not be truly “near”.\n",
        "\n",
        "Model accuracy decreases.\n",
        "\n",
        "Computational cost increases.\n",
        "\n",
        "More data is required to maintain performance.\n",
        "\n",
        "Noise increases in high dimensions.\n",
        "\n",
        "Model overfits easily.\n",
        "\n",
        "Effect on KNN:\n",
        "\n",
        "Distance metrics lose discriminative power.\n",
        "\n",
        "Poor neighbor selection.\n",
        "\n",
        "Increased misclassification.\n",
        "\n",
        "Slower prediction time.\n",
        "\n",
        "Higher memory usage.\n",
        "\n",
        "Reduced generalization.\n",
        "\n",
        "Scaling alone is insufficient.\n",
        "\n",
        "Feature selection or reduction needed.\n",
        "\n",
        "PCA helps mitigate this problem.\n",
        "\n",
        "Dimensionality reduction improves KNN performance.\n",
        "\n",
        "Question 3:\n",
        "What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space.\n",
        "\n",
        "What PCA Does:\n",
        "\n",
        "Converts correlated features into uncorrelated components.\n",
        "\n",
        "Each component is a linear combination of original features.\n",
        "\n",
        "Components are ordered by variance.\n",
        "\n",
        "First component captures maximum variance.\n",
        "\n",
        "Reduces noise and redundancy.\n",
        "\n",
        "Improves model performance.\n",
        "\n",
        "Reduces overfitting.\n",
        "\n",
        "Improves visualization.\n",
        "\n",
        "Speeds up computation.\n",
        "\n",
        "Retains most information.\n",
        "\n",
        "Difference Between PCA and Feature Selection:\n",
        "Aspect\tPCA\tFeature Selection\n",
        "Type\tFeature extraction\tFeature filtering\n",
        "Features\tCreates new features\tKeeps original features\n",
        "Interpretability\tLow\tHigh\n",
        "Correlation\tRemoves correlation\tKeeps correlation\n",
        "Supervision\tUnsupervised\tCan be supervised\n",
        "Dimensionality\tReduced\tReduced\n",
        "Information loss\tPossible\tMinimal\n",
        "Example\tEigenvectors\tSelect top features\n",
        "Data transform\tYes\tNo\n",
        "Use case\tHigh dimensions\tInterpretability\n",
        "Question 4:\n",
        " What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Eigenvalues and eigenvectors are mathematical concepts used in PCA to identify important directions in data.\n",
        "\n",
        "Eigenvectors:\n",
        "\n",
        "Represent directions of maximum variance.\n",
        "\n",
        "Define new feature axes.\n",
        "\n",
        "Are orthogonal to each other.\n",
        "\n",
        "Used to form principal components.\n",
        "\n",
        "Capture relationships between features.\n",
        "\n",
        "Ordered by importance.\n",
        "\n",
        "Reduce correlation.\n",
        "\n",
        "Used for projection.\n",
        "\n",
        "Define transformation matrix.\n",
        "\n",
        "Determine new coordinate system.\n",
        "\n",
        "Eigenvalues:\n",
        "\n",
        "Represent magnitude of variance.\n",
        "\n",
        "Measure importance of eigenvectors.\n",
        "\n",
        "Larger eigenvalue → more information.\n",
        "\n",
        "Used to rank components.\n",
        "\n",
        "Help decide number of components.\n",
        "\n",
        "Used in explained variance ratio.\n",
        "\n",
        "Indicate data spread.\n",
        "\n",
        "Help discard noise.\n",
        "\n",
        "Control dimensionality reduction.\n",
        "\n",
        "Key to PCA efficiency.\n",
        "\n",
        "Question 5:\n",
        "How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "Answer:\n",
        "\n",
        "KNN and PCA work together to improve performance and efficiency.\n",
        "\n",
        "How They Complement Each Other:\n",
        "\n",
        "PCA reduces dimensionality.\n",
        "\n",
        "KNN suffers in high dimensions.\n",
        "\n",
        "PCA removes redundant features.\n",
        "\n",
        "Distance calculations become meaningful.\n",
        "\n",
        "Noise is reduced.\n",
        "\n",
        "Model becomes faster.\n",
        "\n",
        "Memory usage decreases.\n",
        "\n",
        "Overfitting is reduced.\n",
        "\n",
        "Accuracy often improves.\n",
        "\n",
        "Pipeline becomes robust.\n",
        "\n",
        "Overall Benefits:\n",
        "\n",
        "Better generalization.\n",
        "\n",
        "Faster prediction.\n",
        "\n",
        "Improved accuracy.\n",
        "\n",
        "Reduced curse of dimensionality.\n",
        "\n",
        "Suitable for real-world data.\n",
        "\n",
        "Dataset: Wine Dataset\n",
        "Question 6:\n",
        "Train a KNN Classifier on the Wine dataset with and without feature scaling\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Without scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "acc_without_scaling = accuracy_score(y_test, knn.predict(X_test))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "acc_with_scaling = accuracy_score(y_test, knn.predict(X_test_scaled))\n",
        "\n",
        "print(\"Accuracy without scaling:\", acc_without_scaling)\n",
        "print(\"Accuracy with scaling:\", acc_with_scaling)\n",
        "\n",
        "\n",
        "Output:\n",
        "\n",
        "Accuracy without scaling: 0.72\n",
        "Accuracy with scaling: 0.97\n",
        "\n",
        "Question 7:\n",
        "Train a PCA model and print explained variance ratio\n",
        "\n",
        "Answer:\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
        "\n",
        "\n",
        "Output (sample):\n",
        "\n",
        "[0.36 0.19 0.11 0.07 0.06 ...]\n",
        "\n",
        "Question 8: Train KNN on PCA-transformed dataset (top 2 components)\n",
        "\n",
        "Answer:\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "accuracy = accuracy_score(y_test, knn.predict(X_test))\n",
        "print(\"Accuracy with PCA:\", accuracy)\n",
        "\n",
        "\n",
        "Output:\n",
        "\n",
        "Accuracy with PCA: 0.94\n",
        "\n",
        "Question 9:\n",
        "Train KNN with Euclidean and Manhattan distance metrics\n",
        "\n",
        "Answer:\n",
        "\n",
        "knn_euclidean = KNeighborsClassifier(metric='euclidean')\n",
        "knn_manhattan = KNeighborsClassifier(metric='manhattan')\n",
        "\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "\n",
        "acc_euclidean = accuracy_score(y_test, knn_euclidean.predict(X_test_scaled))\n",
        "acc_manhattan = accuracy_score(y_test, knn_manhattan.predict(X_test_scaled))\n",
        "\n",
        "print(\"Euclidean Accuracy:\", acc_euclidean)\n",
        "print(\"Manhattan Accuracy:\", acc_manhattan)\n",
        "\n",
        "\n",
        "Output:\n",
        "\n",
        "Euclidean Accuracy: 0.97\n",
        "Manhattan Accuracy: 0.95\n",
        "\n",
        "Question 10:\n",
        " Gene Expression Dataset – PCA + KNN Pipeline Explanation\n",
        "\n",
        "Answer:\n",
        "\n",
        "Using PCA to Reduce Dimensionality:\n",
        "\n",
        "Gene datasets have thousands of features.\n",
        "\n",
        "PCA reduces feature space.\n",
        "\n",
        "Removes noise and redundancy.\n",
        "\n",
        "Retains major variance.\n",
        "\n",
        "Improves model stability.\n",
        "\n",
        "Deciding Number of Components:\n",
        "\n",
        "Use explained variance ratio.\n",
        "\n",
        "Retain 90–95% variance.\n",
        "\n",
        "Use scree plot.\n",
        "\n",
        "Balance information and simplicity.\n",
        "\n",
        "Avoid over-compression.\n",
        "\n",
        "Using KNN after PCA:\n",
        "\n",
        "Reduced dimensions improve distance accuracy.\n",
        "\n",
        "Faster neighbor search.\n",
        "\n",
        "Less overfitting.\n",
        "\n",
        "Improved generalization.\n",
        "\n",
        "Better classification.\n",
        "\n",
        "Model Evaluation:\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Precision\n",
        "\n",
        "Recall\n",
        "\n",
        "F1-score\n",
        "\n",
        "Cross-validation\n",
        "\n",
        "Justifying to Stakeholders:\n",
        "\n",
        "PCA reduces complexity.\n",
        "\n",
        "Improves robustness.\n",
        "\n",
        "Handles small sample size.\n",
        "\n",
        "Reduces overfitting.\n",
        "\n",
        "Interpretable workflow.\n",
        "\n",
        "Proven ML techniques.\n",
        "\n",
        "Computationally efficient.\n",
        "\n",
        "Suitable for biomedical data.\n",
        "\n",
        "Scientifically justified.\n",
        "\n",
        "Reliable real-world performance."
      ],
      "metadata": {
        "id": "_U5uux4t_eqv"
      }
    }
  ]
}