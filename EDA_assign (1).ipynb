{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " **machine lerning EDA assignment**\n",
        "ques1.\n",
        " 1. Artificial Intelligence (AI)\n",
        "\n",
        "AI is a broad concept of creating machines that can think and act like humans.\n",
        "\n",
        "It aims to simulate human intelligence such as reasoning, learning, decision-making, and problem-solving.\n",
        "\n",
        "AI systems can perform tasks like speech recognition, image recognition, game playing, and decision making.\n",
        "\n",
        "It includes both rule-based systems and learning-based systems.\n",
        "\n",
        "Example: Chatbots, virtual assistants, self-driving cars.\n",
        "\n",
        "2. Machine Learning (ML)\n",
        "\n",
        "ML is a subset of AI.\n",
        "\n",
        "It focuses on enabling machines to learn from data without being explicitly programmed.\n",
        "\n",
        "ML algorithms improve performance automatically with experience.\n",
        "\n",
        "It uses statistical techniques to identify patterns and relationships in data.\n",
        "\n",
        "Example: Email spam detection, recommendation systems.\n",
        "\n",
        "3. Deep Learning (DL)\n",
        "\n",
        "DL is a subset of Machine Learning.\n",
        "\n",
        "It uses artificial neural networks with multiple layers (deep neural networks).\n",
        "\n",
        "DL can automatically extract complex features from large amounts of data.\n",
        "\n",
        "It is especially effective for image, speech, and natural language processing.\n",
        "\n",
        "Example: Face recognition, voice assistants.\n",
        "\n",
        "4. Data Science\n",
        "\n",
        "Data Science is an interdisciplinary field that focuses on extracting useful insights from data.\n",
        "\n",
        "It involves data collection, data cleaning, analysis, visualization, and interpretation.\n",
        "\n",
        "It uses tools and techniques from statistics, ML, AI, and programming.\n",
        "\n",
        "Data Science helps in decision-making and business intelligence.\n",
        "\n",
        "Example: Sales forecasting, customer behavior analysis.\n",
        "ques2----\n",
        "1. Overfitting\n",
        "\n",
        "Definition:\n",
        "\n",
        "Overfitting occurs when a model learns the training data too well, including noise and irrelevant details.\n",
        "\n",
        "The model performs very well on training data but poorly on unseen (test) data.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "High training accuracy.\n",
        "\n",
        "Low test/validation accuracy.\n",
        "\n",
        "Model is too complex for the given data.\n",
        "\n",
        "Detection of Overfitting\n",
        "\n",
        "Large gap between training accuracy and test accuracy.\n",
        "\n",
        "Training error is very low, but validation error is high.\n",
        "\n",
        "Poor performance on new or real-world data.\n",
        "\n",
        "Prevention of Overfitting\n",
        "\n",
        "Use more training data.\n",
        "\n",
        "Apply regularization techniques (L1, L2).\n",
        "\n",
        "Use cross-validation.\n",
        "\n",
        "Reduce model complexity.\n",
        "\n",
        "Use early stopping during training.\n",
        "\n",
        "Apply dropout (in deep learning).\n",
        "\n",
        "2. Underfitting\n",
        "\n",
        "Definition:\n",
        "\n",
        "Underfitting occurs when a model is too simple to capture the underlying pattern of the data.\n",
        "\n",
        "The model performs poorly on both training and test data.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Low training accuracy.\n",
        "\n",
        "Low test/validation accuracy.\n",
        "\n",
        "Model fails to learn meaningful relationships.\n",
        "\n",
        "Detection of Underfitting\n",
        "\n",
        "High training error.\n",
        "\n",
        "High test/validation error.\n",
        "\n",
        "No significant improvement with more training data.\n",
        "\n",
        "Prevention of Underfitting\n",
        "\n",
        "Increase model complexity.\n",
        "\n",
        "Train the model for more epochs.\n",
        "\n",
        "Add more relevant features.\n",
        "\n",
        "Reduce excessive regularization.\n",
        "\n",
        "Use more powerful algorithms.\n",
        "ques3---\n",
        "1. Deletion Method\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Remove rows or columns that contain missing values.\n",
        "\n",
        "Suitable when the number of missing values is very small.\n",
        "\n",
        "Types:\n",
        "\n",
        "Row-wise deletion: Remove records with missing data.\n",
        "\n",
        "Column-wise deletion: Remove features with many missing values.\n",
        "\n",
        "Example:\n",
        "\n",
        "If only 2–3 rows have missing salary values in a dataset of 10,000 records, those rows can be removed.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Simple and fast.\n",
        "\n",
        "No assumptions about data distribution.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Loss of data.\n",
        "\n",
        "Not suitable when many values are missing.\n",
        "\n",
        "2. Mean / Median / Mode Imputation\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Replace missing values with:\n",
        "\n",
        "Mean for numerical data.\n",
        "\n",
        "Median for skewed numerical data.\n",
        "\n",
        "Mode for categorical data.\n",
        "\n",
        "Example:\n",
        "\n",
        "Missing age values are replaced with the average age of all employees.\n",
        "\n",
        "Missing gender values are replaced with the most frequent category.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Easy to implement.\n",
        "\n",
        "Maintains dataset size.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Reduces data variability.\n",
        "\n",
        "May introduce bias.\n",
        "\n",
        "3. Forward Fill / Backward Fill\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Replace missing values using previous or next available values.\n",
        "\n",
        "Mostly used in time-series data.\n",
        "\n",
        "Example:\n",
        "\n",
        "If stock price is missing on Tuesday, use Monday’s price (forward fill).\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Preserves trend in time-based data.\n",
        "\n",
        "Simple approach.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Not suitable for non-sequential data.\n",
        "\n",
        "May propagate incorrect values.\n",
        "ques4\n",
        "1. Imbalanced Dataset\n",
        "\n",
        "Definition:\n",
        "\n",
        "An imbalanced dataset is a dataset in which classes are not equally represented.\n",
        "\n",
        "One class (majority class) has significantly more samples than the other class (minority class).\n",
        "\n",
        "Common in classification problems.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Fraud detection (fraud cases are very few).\n",
        "\n",
        "Disease detection (patients with disease are less than healthy patients).\n",
        "\n",
        "Spam detection (spam emails are fewer than normal emails).\n",
        "\n",
        "Problems Caused:\n",
        "\n",
        "Model becomes biased toward the majority class.\n",
        "\n",
        "High overall accuracy but poor minority-class prediction.\n",
        "\n",
        "Misleading evaluation results.\n",
        "\n",
        "2. Technique 1: Oversampling the Minority Class\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Increase the number of samples in the minority class.\n",
        "\n",
        "Can be done by duplicating existing samples or creating synthetic data.\n",
        "\n",
        "Example:\n",
        "\n",
        "In a fraud dataset, fraud cases are duplicated or synthetically generated to match non-fraud cases.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "No loss of data.\n",
        "\n",
        "Improves minority class learning.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Risk of overfitting.\n",
        "\n",
        "Increases dataset size.\n",
        "\n",
        "3. Technique 2: Undersampling the Majority Class\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Reduce the number of samples in the majority class.\n",
        "\n",
        "Randomly remove some majority class data points.\n",
        "\n",
        "Example:\n",
        "\n",
        "In spam detection, reduce the number of non-spam emails to balance classes.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Faster training.\n",
        "\n",
        "Reduces model bias.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Loss of useful information.\n",
        "\n",
        "May reduce model performance.\n",
        "ques5---\n",
        "1. Feature Scaling\n",
        "\n",
        "Definition:\n",
        "\n",
        "Feature scaling is the process of bringing all input features to a similar range.\n",
        "\n",
        "It ensures that no feature dominates others due to its large numerical value.\n",
        "\n",
        "Why Feature Scaling is Important:\n",
        "\n",
        "Improves model performance and accuracy.\n",
        "\n",
        "Speeds up model convergence during training.\n",
        "\n",
        "Required for distance-based algorithms.\n",
        "\n",
        "Helps gradient-based algorithms work efficiently.\n",
        "\n",
        "Prevents bias caused by features with larger scales.\n",
        "\n",
        "Algorithms that Require Scaling:\n",
        "\n",
        "K-Nearest Neighbors (KNN)\n",
        "\n",
        "Support Vector Machines (SVM)\n",
        "\n",
        "Linear and Logistic Regression\n",
        "\n",
        "Neural Networks\n",
        "\n",
        "K-Means Clustering\n",
        "\n",
        "2. Min-Max Scaling\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Rescales features to a fixed range, usually 0 to 1.\n",
        "\n",
        "\n",
        "\tCharacteristics:\n",
        "\n",
        "Preserves original data distribution.\n",
        "\n",
        "Sensitive to outliers.\n",
        "\n",
        "Example:\n",
        "\n",
        "If age ranges from 18 to 60, Min-Max scaling converts it to values between 0 and 1.\n",
        "Question 6: Compare Label Encoding and One-Hot Encoding. When would you prefer one over the other?\n",
        "Label Encoding\n",
        "\n",
        "Converts categorical values into integer numbers.\n",
        "\n",
        "Each category is assigned a unique numeric label.\n",
        "\n",
        "Suitable for ordinal data (where order matters).\n",
        "\n",
        "Example:\n",
        "\n",
        "Education Level:  Low, Medium, High\n",
        "Encoded as:       0,   1,      2\n",
        "\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Simple and memory efficient.\n",
        "\n",
        "Preserves order in ordinal variables.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Can introduce false numerical relationships in nominal data.\n",
        "\n",
        "One-Hot Encoding\n",
        "\n",
        "Converts categories into binary (0/1) columns.\n",
        "\n",
        "No order is assumed among categories.\n",
        "\n",
        "Best for nominal data.\n",
        "\n",
        "Example:\n",
        "\n",
        "Color → Red, Blue, Green\n",
        "Red   → 1 0 0\n",
        "Blue  → 0 1 0\n",
        "Green → 0 0 1\n",
        "\n",
        "\n",
        "Advantages:\n",
        "\n",
        "No misleading order.\n",
        "\n",
        "Works well with most ML models.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Increases dimensionality.\n",
        "\n",
        "Not efficient for very high-cardinality features.\n",
        "\n",
        "Comparison Table\n",
        "Aspect\tLabel Encoding\tOne-Hot Encoding\n",
        "Data Type\tOrdinal\tNominal\n",
        "Order Preserved\tYes\tNo\n",
        "Columns Created\tOne\tMultiple\n",
        "Risk of Bias\tHigh (for nominal)\tLow\n",
        "Conclusion\n",
        "\n",
        "Use Label Encoding for ordinal variables (ranked).\n",
        "\n",
        "Use One-Hot Encoding for nominal variables (no order).\n",
        "\n",
        "Question 7:\n",
        " Google Play Store Dataset\n",
        "a) Relationship between App Categories and Ratings\n",
        "Python Code\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"googleplaystore.csv\")\n",
        "\n",
        "# Clean ratings\n",
        "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
        "\n",
        "category_ratings = df.groupby('Category')['Rating'].mean().sort_values(ascending=False)\n",
        "print(category_ratings)\n",
        "\n",
        "Sample Output\n",
        "Category\n",
        "EDUCATION            4.40\n",
        "EVENTS               4.39\n",
        "ART_AND_DESIGN       4.35\n",
        "BOOKS_AND_REFERENCE  4.34\n",
        "...\n",
        "DATING               3.90\n",
        "TOOLS                3.88\n",
        "\n",
        "Analysis\n",
        "\n",
        "Highest Rated Categories:\n",
        "\n",
        "Education\n",
        "\n",
        "Events\n",
        "\n",
        "Books & Reference\n",
        "\n",
        "Lowest Rated Categories:\n",
        "\n",
        "Dating\n",
        "\n",
        "Tools\n",
        "\n",
        "Possible Reasons\n",
        "\n",
        "Educational apps focus on content quality.\n",
        "\n",
        "Dating apps face user dissatisfaction and fake profiles.\n",
        "\n",
        "Utility apps face high competition.\n",
        "\n",
        "Question 8:\n",
        " Titanic Dataset\n",
        "a) Survival Rate by Passenger Class\n",
        "Python Code\n",
        "df = pd.read_csv(\"titanic.csv\")\n",
        "\n",
        "survival_by_class = df.groupby('Pclass')['Survived'].mean()\n",
        "print(survival_by_class)\n",
        "\n",
        "Sample Output\n",
        "Pclass\n",
        "1    0.63\n",
        "2    0.47\n",
        "3    0.24\n",
        "\n",
        "Conclusion\n",
        "\n",
        "1st Class passengers had the highest survival rate.\n",
        "\n",
        "Due to priority evacuation, wealth, and cabin location.\n",
        "\n",
        "b) Survival Based on Age\n",
        "Python Code\n",
        "df['AgeGroup'] = df['Age'].apply(lambda x: 'Child' if x < 18 else 'Adult')\n",
        "\n",
        "age_survival = df.groupby('AgeGroup')['Survived'].mean()\n",
        "print(age_survival)\n",
        "\n",
        "Sample Output\n",
        "AgeGroup\n",
        "Child    0.54\n",
        "Adult    0.38\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Children had better survival chances.\n",
        "\n",
        "Followed the “Women and Children First” rule.\n",
        "\n",
        "Question 9:\n",
        "Flight Price Prediction Dataset\n",
        "a) Prices vs Days Left for Departure\n",
        "Python Code\n",
        "df = pd.read_csv(\"flight_price.csv\")\n",
        "\n",
        "price_trend = df.groupby('Days_left')['Price'].mean()\n",
        "print(price_trend.head())\n",
        "\n",
        "Sample Output\n",
        "Days_left\n",
        "1     8500\n",
        "2     8100\n",
        "5     7200\n",
        "10    6500\n",
        "30    4800\n",
        "\n",
        "Analysis\n",
        "\n",
        "Prices increase sharply as departure date approaches.\n",
        "\n",
        "Exponential surge seen in last 5–7 days.\n",
        "\n",
        "Best Booking Window\n",
        "\n",
        "20–40 days before departure.\n",
        "\n",
        "b) Price Comparison Across Airlines\n",
        "Python Code\n",
        "route_df = df[df['Route'] == 'Delhi-Mumbai']\n",
        "\n",
        "airline_prices = route_df.groupby('Airline')['Price'].mean()\n",
        "print(airline_prices)\n",
        "\n",
        "Sample Output\n",
        "Airline\n",
        "IndiGo        5200\n",
        "Air India    6100\n",
        "Vistara      6800\n",
        "\n",
        "Conclusion\n",
        "\n",
        "IndiGo → consistently cheaper (budget airline).\n",
        "\n",
        "Vistara → premium pricing due to service quality.\n",
        "\n",
        "Question 10:\n",
        "HR Analytics Dataset\n",
        "a) Factors Affecting Employee Attrition\n",
        "Python Code\n",
        "df = pd.read_csv(\"HR_comma_sep.csv\")\n",
        "\n",
        "attrition_corr = df.corr()['left'].sort_values(ascending=False)\n",
        "print(attrition_corr)\n",
        "\n",
        "Sample Output\n",
        "left\n",
        "satisfaction_level   -0.39\n",
        "time_spend_company    0.14\n",
        "average_montly_hours  0.12\n",
        "\n",
        "Key Drivers\n",
        "\n",
        "Low satisfaction → high attrition.\n",
        "\n",
        "High overtime → burnout.\n",
        "\n",
        "Low salary → higher resignation.\n",
        "\n",
        "b) Projects vs Attrition\n",
        "Python Code\n",
        "project_attrition = df.groupby('number_project')['left'].mean()\n",
        "print(project_attrition)\n",
        "\n",
        "Sample Output\n",
        "number_project\n",
        "2    0.18\n",
        "4    0.22\n",
        "6    0.55"
      ],
      "metadata": {
        "id": "e3fd0ZvvyC9f"
      }
    }
  ]
}